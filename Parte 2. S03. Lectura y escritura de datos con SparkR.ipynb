{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style='font-size:2.4em'>Procesamiento masivo de datos con SparkR</span>\n",
    "\n",
    "<span style='font-size:1.5em'>VIII Jornadas de usuarios de R. Albacete, Castilla-La Mancha, 17 y 18 de noviembre de 2016</span>\n",
    "\n",
    "Taller impartido por: <span style='font-size:1.2em'>Manuel Jesús Parra Royón</span>\n",
    "\n",
    "\n",
    "![Alt](https://sites.google.com/site/manuparra/home/logoparty.png)\n",
    "\n",
    "<HR>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lectura de datos desde SparkR\n",
    "\n",
    "![Spark+R](https://sites.google.com/site/manuparra/home/SparkRlogo.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como siempre, para todos nuestros `scripts` con **SparkR**, cargamos la biblioteca, y creamos una nueva sesión de SparkR."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Attaching package: ‘SparkR’\n",
      "\n",
      "The following objects are masked from ‘package:stats’:\n",
      "\n",
      "    cov, filter, lag, na.omit, predict, sd, var, window\n",
      "\n",
      "The following objects are masked from ‘package:base’:\n",
      "\n",
      "    as.data.frame, colnames, colnames<-, drop, endsWith, intersect,\n",
      "    rank, rbind, sample, startsWith, subset, summary, transform, union\n",
      "\n",
      "Spark package found in SPARK_HOME: /usr/local/spark/\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Launching java with spark-submit command /usr/local/spark//bin/spark-submit   --driver-memory \"1g\" sparkr-shell /tmp/RtmpVALPn5/backend_porta2e676ae652 \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Java ref type org.apache.spark.sql.SparkSession id 1 "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    ".libPaths(c(file.path(Sys.getenv(\"SPARK_HOME\"),\"R/lib/\"),.libPaths()))\n",
    "library(SparkR)\n",
    "sparkR.session(appName=\"EntornoInicio\", master = \"local[*]\", sparkConfig = list(spark.driver.memory = \"1g\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hoy en día el trabajo con BigData parece que siempre está asociado al ecosistema HADOOP. Hace unos años esto significaba que si también eras un buen programador en JAVA, trabajar en tal entorno, incluso un simple programa para hacer un WORDCOUNT, implicaba varias docenas de líneas de código. \n",
    "\n",
    "Pero hace 3-4 años la cuestión ha cambiado gracias a Apache Spark con su API de estilo funcional. \n",
    "\n",
    "Está escrito en SCALA pero también puede ser utilizado desde Python, JAVA y como estais viendo por este Taller: también en R"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fuentes de datos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dentro de una sesión de Spark, las aplicaciones pueden crear `SparkDataFrames` desde variadas fuentes de datos, como por ejemplo:  un fichero local (`data.frame`), desde HDFS (``hdfs:///``),  desde tablas en `HIVE` o desde otras múltiples fuentes de datos (AmazonS3, etc).\n",
    "\n",
    "Concretamente las principales fuentes u orígenes de datos desde las que **cargar datos** son los siguientes:\n",
    "\n",
    "* Ficheros locales\n",
    "* Ficheros en sistemas distribuidos de almacenamiento Hadoop HDFS\n",
    "* Sistemas de almacenamiento de datos tipo HIVE\n",
    "* Desde bases de datos relacionales a través de JDBC\n",
    "* ...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tipos de fuentes de datos\n",
    "\n",
    "Una cosa son las **fuentes de datos** y otras cosa son los **tipo de fuentes de datos**. El tipo de fuente de datos puede ser visto como el formato de los datos.\n",
    "\n",
    "Los conjuntos de datos pueden están almacenados en diferentes formatos, los más utilizados para SparkR (y Spark):\n",
    "\n",
    "* Ficheros planos y CSV\n",
    "* **Ficheros JSON**\n",
    "* Ficheros de tipo ```avro``` (row-based)\n",
    "* Ficheros de tipo ```parquet``` (column-based)\n",
    "* Ficheros de tipo ```orc``` (column-based)\n",
    "\n",
    "![ListOfSources](https://sites.google.com/site/manuparra/home/files_API.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Repositorio de Datasets para todo el taller\n",
    "\n",
    "Todos los conjuntos de datos que vamos a tratar para el Taller se encuentran disponibles en el directorio ```datasets```. Para consultar, modificar y añadir datasets, ficheros, etc, puedes hacerlo usando el gestor de fichero de ```Jupyter``` desde:  http://localhost:25980/tree/datasets\n",
    "\n",
    "<BR>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trabajo con ficheros en formato CSV \n",
    "\n",
    "Vamos a revisar todas las funcionalidades que ofrece SparkR para el manejo de CSV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lectura"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comprobamos que hay en el directorio donde tenemos los datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<ol class=list-inline>\n",
       "\t<li>'/root/TallerSparkR/datasets/csv/buy_05/buy_costumers_amazon05.csv'</li>\n",
       "\t<li>'/root/TallerSparkR/datasets/csv/buy_costumers_amazon01.csv'</li>\n",
       "\t<li>'/root/TallerSparkR/datasets/csv/buy_costumers_amazon02.csv'</li>\n",
       "\t<li>'/root/TallerSparkR/datasets/csv/buy_costumers_amazon03.csv'</li>\n",
       "\t<li>'/root/TallerSparkR/datasets/csv/buy_costumers_amazon04.csv'</li>\n",
       "</ol>\n"
      ],
      "text/latex": [
       "\\begin{enumerate*}\n",
       "\\item '/root/TallerSparkR/datasets/csv/buy\\_05/buy\\_costumers\\_amazon05.csv'\n",
       "\\item '/root/TallerSparkR/datasets/csv/buy\\_costumers\\_amazon01.csv'\n",
       "\\item '/root/TallerSparkR/datasets/csv/buy\\_costumers\\_amazon02.csv'\n",
       "\\item '/root/TallerSparkR/datasets/csv/buy\\_costumers\\_amazon03.csv'\n",
       "\\item '/root/TallerSparkR/datasets/csv/buy\\_costumers\\_amazon04.csv'\n",
       "\\end{enumerate*}\n"
      ],
      "text/markdown": [
       "1. '/root/TallerSparkR/datasets/csv/buy_05/buy_costumers_amazon05.csv'\n",
       "2. '/root/TallerSparkR/datasets/csv/buy_costumers_amazon01.csv'\n",
       "3. '/root/TallerSparkR/datasets/csv/buy_costumers_amazon02.csv'\n",
       "4. '/root/TallerSparkR/datasets/csv/buy_costumers_amazon03.csv'\n",
       "5. '/root/TallerSparkR/datasets/csv/buy_costumers_amazon04.csv'\n",
       "\n",
       "\n"
      ],
      "text/plain": [
       "[1] \"/root/TallerSparkR/datasets/csv/buy_05/buy_costumers_amazon05.csv\"\n",
       "[2] \"/root/TallerSparkR/datasets/csv/buy_costumers_amazon01.csv\"       \n",
       "[3] \"/root/TallerSparkR/datasets/csv/buy_costumers_amazon02.csv\"       \n",
       "[4] \"/root/TallerSparkR/datasets/csv/buy_costumers_amazon03.csv\"       \n",
       "[5] \"/root/TallerSparkR/datasets/csv/buy_costumers_amazon04.csv\"       "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "list.files(\"/root/TallerSparkR/datasets/csv\",full.names = T,recursive = T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Leemos un fichero concreto de datos en formato ```CSV``` del directorio ```datasets/csv```. El fichero de ejemplo sólo tiene 1000 registros:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para la lectura de datos con SparkR usamos la función ``read.df( )``"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: integer (nullable = true)\n",
      " |-- first_name: string (nullable = true)\n",
      " |-- last_name: string (nullable = true)\n",
      " |-- buy_hours: string (nullable = true)\n",
      " |-- amount: double (nullable = true)\n",
      " |-- credit_card: long (nullable = true)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "1000"
      ],
      "text/latex": [
       "1000"
      ],
      "text/markdown": [
       "1000"
      ],
      "text/plain": [
       "[1] 1000"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<thead><tr><th scope=col>id</th><th scope=col>first_name</th><th scope=col>last_name</th><th scope=col>buy_hours</th><th scope=col>amount</th><th scope=col>credit_card</th></tr></thead>\n",
       "<tbody>\n",
       "\t<tr><td>1           </td><td>Julie       </td><td>Knight      </td><td>22:50       </td><td>2865.65     </td><td>3.003823e+13</td></tr>\n",
       "\t<tr><td>2           </td><td>Frank       </td><td>Hernandez   </td><td>23:48       </td><td>3783.93     </td><td>3.577441e+15</td></tr>\n",
       "\t<tr><td>3           </td><td>Andrew      </td><td>Patterson   </td><td>19:32       </td><td> 581.70     </td><td>3.546533e+15</td></tr>\n",
       "\t<tr><td>4           </td><td>Charles     </td><td>Snyder      </td><td>9:44        </td><td>3257.08     </td><td>5.002359e+15</td></tr>\n",
       "\t<tr><td>5           </td><td>Earl        </td><td>Little      </td><td>19:23       </td><td> 439.36     </td><td>5.048378e+15</td></tr>\n",
       "\t<tr><td>6           </td><td>Pamela      </td><td>Torres      </td><td>16:00       </td><td>3773.75     </td><td>3.531928e+15</td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "\\begin{tabular}{r|llllll}\n",
       " id & first\\_name & last\\_name & buy\\_hours & amount & credit\\_card\\\\\n",
       "\\hline\n",
       "\t 1            & Julie        & Knight       & 22:50        & 2865.65      & 3.003823e+13\\\\\n",
       "\t 2            & Frank        & Hernandez    & 23:48        & 3783.93      & 3.577441e+15\\\\\n",
       "\t 3            & Andrew       & Patterson    & 19:32        &  581.70      & 3.546533e+15\\\\\n",
       "\t 4            & Charles      & Snyder       & 9:44         & 3257.08      & 5.002359e+15\\\\\n",
       "\t 5            & Earl         & Little       & 19:23        &  439.36      & 5.048378e+15\\\\\n",
       "\t 6            & Pamela       & Torres       & 16:00        & 3773.75      & 3.531928e+15\\\\\n",
       "\\end{tabular}\n"
      ],
      "text/plain": [
       "  id first_name last_name buy_hours amount  credit_card \n",
       "1 1  Julie      Knight    22:50     2865.65 3.003823e+13\n",
       "2 2  Frank      Hernandez 23:48     3783.93 3.577441e+15\n",
       "3 3  Andrew     Patterson 19:32      581.70 3.546533e+15\n",
       "4 4  Charles    Snyder    9:44      3257.08 5.002359e+15\n",
       "5 5  Earl       Little    19:23      439.36 5.048378e+15\n",
       "6 6  Pamela     Torres    16:00     3773.75 3.531928e+15"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Sólo indicamos un fichero concreto .... No hay problema Spark es muy listo ! ;)\n",
    "df <- read.df(\"/root/TallerSparkR/datasets/csv/buy_costumers_amazon01.csv\", \"csv\", header = \"true\", inferSchema = \"true\")\n",
    "printSchema(df)\n",
    "count(df)\n",
    "head(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] \"Estructura sin parsear:\"\n",
      "root\n",
      " |-- id: integer (nullable = true)\n",
      " |-- first_name: string (nullable = true)\n",
      " |-- last_name: string (nullable = true)\n",
      " |-- buy_hours: string (nullable = true)\n",
      " |-- amount: double (nullable = true)\n",
      " |-- credit_card: long (nullable = true)\n",
      "[1] \"Estructura parseada:\"\n",
      "root\n",
      " |-- id: integer (nullable = true)\n",
      " |-- first_name: string (nullable = true)\n",
      " |-- last_name: string (nullable = true)\n",
      " |-- buy_hours: string (nullable = true)\n",
      " |-- amount: double (nullable = true)\n",
      " |-- credit_card: string (nullable = true)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<thead><tr><th scope=col>id</th><th scope=col>first_name</th><th scope=col>last_name</th><th scope=col>buy_hours</th><th scope=col>amount</th><th scope=col>credit_card</th></tr></thead>\n",
       "<tbody>\n",
       "\t<tr><td>1               </td><td>Julie           </td><td>Knight          </td><td>22:50           </td><td>2865.65         </td><td>30038229243005  </td></tr>\n",
       "\t<tr><td>2               </td><td>Frank           </td><td>Hernandez       </td><td>23:48           </td><td>3783.93         </td><td>3577440681063074</td></tr>\n",
       "\t<tr><td>3               </td><td>Andrew          </td><td>Patterson       </td><td>19:32           </td><td> 581.70         </td><td>3546532606705637</td></tr>\n",
       "\t<tr><td>4               </td><td>Charles         </td><td>Snyder          </td><td>9:44            </td><td>3257.08         </td><td>5002359371334068</td></tr>\n",
       "\t<tr><td>5               </td><td>Earl            </td><td>Little          </td><td>19:23           </td><td> 439.36         </td><td>5048377824454388</td></tr>\n",
       "\t<tr><td>6               </td><td>Pamela          </td><td>Torres          </td><td>16:00           </td><td>3773.75         </td><td>3531927547020434</td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "\\begin{tabular}{r|llllll}\n",
       " id & first\\_name & last\\_name & buy\\_hours & amount & credit\\_card\\\\\n",
       "\\hline\n",
       "\t 1                & Julie            & Knight           & 22:50            & 2865.65          & 30038229243005  \\\\\n",
       "\t 2                & Frank            & Hernandez        & 23:48            & 3783.93          & 3577440681063074\\\\\n",
       "\t 3                & Andrew           & Patterson        & 19:32            &  581.70          & 3546532606705637\\\\\n",
       "\t 4                & Charles          & Snyder           & 9:44             & 3257.08          & 5002359371334068\\\\\n",
       "\t 5                & Earl             & Little           & 19:23            &  439.36          & 5048377824454388\\\\\n",
       "\t 6                & Pamela           & Torres           & 16:00            & 3773.75          & 3531927547020434\\\\\n",
       "\\end{tabular}\n"
      ],
      "text/plain": [
       "  id first_name last_name buy_hours amount  credit_card     \n",
       "1 1  Julie      Knight    22:50     2865.65 30038229243005  \n",
       "2 2  Frank      Hernandez 23:48     3783.93 3577440681063074\n",
       "3 3  Andrew     Patterson 19:32      581.70 3546532606705637\n",
       "4 4  Charles    Snyder    9:44      3257.08 5002359371334068\n",
       "5 5  Earl       Little    19:23      439.36 5048377824454388\n",
       "6 6  Pamela     Torres    16:00     3773.75 3531927547020434"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Sólo indicamos un fichero concreto .... No hay problema Spark es muy listo ! ;)\n",
    "df <- read.df(\"/root/TallerSparkR/datasets/csv/buy_costumers_amazon01.csv\", \"csv\", header = \"true\", inferSchema = \"true\")\n",
    "print(\"Estructura sin parsear:\")\n",
    "printSchema(df)\n",
    "\n",
    "# Creamos un esquema para definir cual será la estructura del fichero a leer.\n",
    "schema_amazon <- structType(structField(\"id\", \"integer\"),\n",
    "                     structField(\"first_name\", \"string\"),\n",
    "                     structField(\"last_name\", \"string\"),\n",
    "                     structField(\"buy_hours\", \"string\"),\n",
    "                     structField(\"amount\", \"double\"),\n",
    "                     structField(\"credit_card\", \"string\"))\n",
    "\n",
    "df <- read.df(\"/root/TallerSparkR/datasets/csv/buy_costumers_amazon01.csv\", \"csv\", header = \"true\", schema=schema_amazon)\n",
    "print(\"Estructura parseada:\")\n",
    "printSchema(df)\n",
    "head(df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si queremos leer todos los ficheros de un directorio sin entrar en los subdirectorios:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "4000"
      ],
      "text/latex": [
       "4000"
      ],
      "text/markdown": [
       "4000"
      ],
      "text/plain": [
       "[1] 4000"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Esto leería todos los ficheros de la carpeta pero no entraría a cada subdirectorio... Spark no eres muy listo !\n",
    "df <- read.df(\"/root/TallerSparkR/datasets/csv/\", \"csv\", header = \"true\", inferSchema = \"true\", schema=schema_amazon)\n",
    "count(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comprobamos que estructura de ficheros y directorios tenemos. Observamos que en ```datasets/csv/``` existen subdirectorios, por lo que hay que usar comodines: *****\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<ol class=list-inline>\n",
       "\t<li>'/root/TallerSparkR/datasets/csv/buy_05/buy_costumers_amazon05.csv'</li>\n",
       "\t<li>'/root/TallerSparkR/datasets/csv/buy_costumers_amazon01.csv'</li>\n",
       "\t<li>'/root/TallerSparkR/datasets/csv/buy_costumers_amazon02.csv'</li>\n",
       "\t<li>'/root/TallerSparkR/datasets/csv/buy_costumers_amazon03.csv'</li>\n",
       "\t<li>'/root/TallerSparkR/datasets/csv/buy_costumers_amazon04.csv'</li>\n",
       "</ol>\n"
      ],
      "text/latex": [
       "\\begin{enumerate*}\n",
       "\\item '/root/TallerSparkR/datasets/csv/buy\\_05/buy\\_costumers\\_amazon05.csv'\n",
       "\\item '/root/TallerSparkR/datasets/csv/buy\\_costumers\\_amazon01.csv'\n",
       "\\item '/root/TallerSparkR/datasets/csv/buy\\_costumers\\_amazon02.csv'\n",
       "\\item '/root/TallerSparkR/datasets/csv/buy\\_costumers\\_amazon03.csv'\n",
       "\\item '/root/TallerSparkR/datasets/csv/buy\\_costumers\\_amazon04.csv'\n",
       "\\end{enumerate*}\n"
      ],
      "text/markdown": [
       "1. '/root/TallerSparkR/datasets/csv/buy_05/buy_costumers_amazon05.csv'\n",
       "2. '/root/TallerSparkR/datasets/csv/buy_costumers_amazon01.csv'\n",
       "3. '/root/TallerSparkR/datasets/csv/buy_costumers_amazon02.csv'\n",
       "4. '/root/TallerSparkR/datasets/csv/buy_costumers_amazon03.csv'\n",
       "5. '/root/TallerSparkR/datasets/csv/buy_costumers_amazon04.csv'\n",
       "\n",
       "\n"
      ],
      "text/plain": [
       "[1] \"/root/TallerSparkR/datasets/csv/buy_05/buy_costumers_amazon05.csv\"\n",
       "[2] \"/root/TallerSparkR/datasets/csv/buy_costumers_amazon01.csv\"       \n",
       "[3] \"/root/TallerSparkR/datasets/csv/buy_costumers_amazon02.csv\"       \n",
       "[4] \"/root/TallerSparkR/datasets/csv/buy_costumers_amazon03.csv\"       \n",
       "[5] \"/root/TallerSparkR/datasets/csv/buy_costumers_amazon04.csv\"       "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "list.files(\"/root/TallerSparkR/datasets/csv\",full.names = T,recursive = T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ejecutamos la siguiente instrucción para poder leer todo lo que cuelgue del directorio donde están los CSV."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Leemos todos los ficheros CSV que haya en el directorio y todo en las subcarpetas... Spark que bien, no?\n",
    "df_full <- read.df(\"/root/TallerSparkR/datasets/csv/*/\", \"csv\", header = \"true\", inferSchema = \"true\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Verificamos que ahora tenemos todos los datos cargados desde todos los ficheros ```CSV``` de la estructura de directorios:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "5000"
      ],
      "text/latex": [
       "5000"
      ],
      "text/markdown": [
       "5000"
      ],
      "text/plain": [
       "[1] 5000"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "count(df_full)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Escritura\n",
    "\n",
    "Una vez que hemos realizado transformaciones con los datos del SparkDataFrame, podemos dejarlo en memoria o bien pasarlo a DISCO (local) o HDFS (distribuido).\n",
    "\n",
    "La API de fuentes de datos puede también ser usada para guardar y almacenar ``SparkDataFrames`` en múltiples formatos. Por ejemplo podemos almacenar el ``SparkDataDrame`` desde/hacia otros formatos como ``CSV``, ``PARQUET`` usando la función ``write.df``. \n",
    "\n",
    "Esto da mucha versatilidad, ya que independiente del tipo de fuente, podemos almacenarlo y leerlo desde cualquiera otra fuente. Como no podía ser de otra forma."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Escritura desde CSV a CSV:\n",
    "write.df(df_full, path = \"datasets/results/df_full.csv\", source = \"csv\", mode = \"overwrite\")\n",
    "\n",
    "# Escritura desde CSV a PARQUET\n",
    "write.df(df_full, path = \"datasets/results/df_full.parquet\", source = \"parquet\", mode = \"overwrite\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En ``mode``\tpodemos usar 'append', 'overwrite', 'error', 'ignore'."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## PARQUET"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parquet es un formato de **almacenamiento en columnas** disponible para cualquier proyecto dentro del ecosistema de Hadoop, enfocado en la mejora del procesamiento de datos, modelado de datos y programación.\n",
    "\n",
    "Parquet está diseñado para soportar esquemas de compresión y codificación muy eficientes. Múltiples proyectos han demostrado el impacto en el rendimiento de aplicar el correcto sistema de compresión y codificación a los datos. Parquet permite que los esquemas de compresión se especifiquen a nivel de columna.\n",
    "\n",
    "Es un formato bien estructurado para ser usado **para problemas de BigData**.\n",
    "\n",
    "La estructura del fichero se **segmenta en N columnas partidas en M grupos de filas**:\n",
    "\n",
    "![Alt](https://raw.github.com/Parquet/parquet-format/master/doc/images/FileLayout.gif)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Leemos el dataset en formato Parquet, luego el resultado de la lectura es un SparkDataFrame, compatible con el trabajo en SparkR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Leemos un dataset que contiene los datos en formato Parquet\n",
    "df_parquet <- read.df(\"/root/TallerSparkR/datasets/parquet/\", \"parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- user_id: string (nullable = true)\n",
      " |-- category: string (nullable = true)\n",
      " |-- variable1: double (nullable = true)\n",
      " |-- variable2: double (nullable = true)\n",
      " |-- variable3: double (nullable = true)\n"
     ]
    }
   ],
   "source": [
    "# Vemos la estructura del fichero y sus atributos\n",
    "printSchema(df_parquet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<thead><tr><th scope=col>user_id</th><th scope=col>category</th><th scope=col>variable1</th><th scope=col>variable2</th><th scope=col>variable3</th></tr></thead>\n",
       "<tbody>\n",
       "\t<tr><td>609f2d681b994fd6af8ed1e62b3be6e4</td><td>Shopping                        </td><td>13.445387                       </td><td> 8.605127                       </td><td> 8.091069                       </td></tr>\n",
       "\t<tr><td>96cf2de06d520133d9604f513dc22504</td><td>Advertising                     </td><td> 9.076202                       </td><td>11.514858                       </td><td> 6.252373                       </td></tr>\n",
       "\t<tr><td>e0669c33ee079c43bc22f2f2bb615ccb</td><td>News                            </td><td>10.806771                       </td><td> 9.578176                       </td><td> 5.613295                       </td></tr>\n",
       "\t<tr><td>3d9d21ea525895a464f4eab75f03cee9</td><td>Arts_and_Entertainment          </td><td> 9.877052                       </td><td> 5.284544                       </td><td> 8.680847                       </td></tr>\n",
       "\t<tr><td>69a728ead2a5219337f59a5909561d02</td><td>Science                         </td><td> 6.085140                       </td><td>15.951622                       </td><td>11.223350                       </td></tr>\n",
       "\t<tr><td>9ac8c3edbfc4a5aa340d91886a73d588</td><td>Advertising                     </td><td> 9.253095                       </td><td> 4.606095                       </td><td>10.748271                       </td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "\\begin{tabular}{r|lllll}\n",
       " user\\_id & category & variable1 & variable2 & variable3\\\\\n",
       "\\hline\n",
       "\t 609f2d681b994fd6af8ed1e62b3be6e4 & Shopping                         & 13.445387                        &  8.605127                        &  8.091069                       \\\\\n",
       "\t 96cf2de06d520133d9604f513dc22504 & Advertising                      &  9.076202                        & 11.514858                        &  6.252373                       \\\\\n",
       "\t e0669c33ee079c43bc22f2f2bb615ccb & News                             & 10.806771                        &  9.578176                        &  5.613295                       \\\\\n",
       "\t 3d9d21ea525895a464f4eab75f03cee9 & Arts\\_and\\_Entertainment       &  9.877052                        &  5.284544                        &  8.680847                       \\\\\n",
       "\t 69a728ead2a5219337f59a5909561d02 & Science                          &  6.085140                        & 15.951622                        & 11.223350                       \\\\\n",
       "\t 9ac8c3edbfc4a5aa340d91886a73d588 & Advertising                      &  9.253095                        &  4.606095                        & 10.748271                       \\\\\n",
       "\\end{tabular}\n"
      ],
      "text/plain": [
       "  user_id                          category               variable1 variable2\n",
       "1 609f2d681b994fd6af8ed1e62b3be6e4 Shopping               13.445387  8.605127\n",
       "2 96cf2de06d520133d9604f513dc22504 Advertising             9.076202 11.514858\n",
       "3 e0669c33ee079c43bc22f2f2bb615ccb News                   10.806771  9.578176\n",
       "4 3d9d21ea525895a464f4eab75f03cee9 Arts_and_Entertainment  9.877052  5.284544\n",
       "5 69a728ead2a5219337f59a5909561d02 Science                 6.085140 15.951622\n",
       "6 9ac8c3edbfc4a5aa340d91886a73d588 Advertising             9.253095  4.606095\n",
       "  variable3\n",
       "1  8.091069\n",
       "2  6.252373\n",
       "3  5.613295\n",
       "4  8.680847\n",
       "5 11.223350\n",
       "6 10.748271"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Vemos un resumen de los datos del fichero ...\n",
    "head(df_parquet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Hacemos un pequeño cambio en el nombre de las columnas del SparkDataFrame.\n",
    "colnames(df_parquet) <- c(\"user_id\",\"cat\",\"R1\",\"R2\",\"R3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vemos de nuevo el cambio de las columnas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<thead><tr><th scope=col>user_id</th><th scope=col>cat</th><th scope=col>R1</th><th scope=col>R2</th><th scope=col>R3</th></tr></thead>\n",
       "<tbody>\n",
       "\t<tr><td>609f2d681b994fd6af8ed1e62b3be6e4</td><td>Shopping                        </td><td>13.445387                       </td><td> 8.605127                       </td><td> 8.091069                       </td></tr>\n",
       "\t<tr><td>96cf2de06d520133d9604f513dc22504</td><td>Advertising                     </td><td> 9.076202                       </td><td>11.514858                       </td><td> 6.252373                       </td></tr>\n",
       "\t<tr><td>e0669c33ee079c43bc22f2f2bb615ccb</td><td>News                            </td><td>10.806771                       </td><td> 9.578176                       </td><td> 5.613295                       </td></tr>\n",
       "\t<tr><td>3d9d21ea525895a464f4eab75f03cee9</td><td>Arts_and_Entertainment          </td><td> 9.877052                       </td><td> 5.284544                       </td><td> 8.680847                       </td></tr>\n",
       "\t<tr><td>69a728ead2a5219337f59a5909561d02</td><td>Science                         </td><td> 6.085140                       </td><td>15.951622                       </td><td>11.223350                       </td></tr>\n",
       "\t<tr><td>9ac8c3edbfc4a5aa340d91886a73d588</td><td>Advertising                     </td><td> 9.253095                       </td><td> 4.606095                       </td><td>10.748271                       </td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "\\begin{tabular}{r|lllll}\n",
       " user\\_id & cat & R1 & R2 & R3\\\\\n",
       "\\hline\n",
       "\t 609f2d681b994fd6af8ed1e62b3be6e4 & Shopping                         & 13.445387                        &  8.605127                        &  8.091069                       \\\\\n",
       "\t 96cf2de06d520133d9604f513dc22504 & Advertising                      &  9.076202                        & 11.514858                        &  6.252373                       \\\\\n",
       "\t e0669c33ee079c43bc22f2f2bb615ccb & News                             & 10.806771                        &  9.578176                        &  5.613295                       \\\\\n",
       "\t 3d9d21ea525895a464f4eab75f03cee9 & Arts\\_and\\_Entertainment       &  9.877052                        &  5.284544                        &  8.680847                       \\\\\n",
       "\t 69a728ead2a5219337f59a5909561d02 & Science                          &  6.085140                        & 15.951622                        & 11.223350                       \\\\\n",
       "\t 9ac8c3edbfc4a5aa340d91886a73d588 & Advertising                      &  9.253095                        &  4.606095                        & 10.748271                       \\\\\n",
       "\\end{tabular}\n"
      ],
      "text/plain": [
       "  user_id                          cat                    R1        R2       \n",
       "1 609f2d681b994fd6af8ed1e62b3be6e4 Shopping               13.445387  8.605127\n",
       "2 96cf2de06d520133d9604f513dc22504 Advertising             9.076202 11.514858\n",
       "3 e0669c33ee079c43bc22f2f2bb615ccb News                   10.806771  9.578176\n",
       "4 3d9d21ea525895a464f4eab75f03cee9 Arts_and_Entertainment  9.877052  5.284544\n",
       "5 69a728ead2a5219337f59a5909561d02 Science                 6.085140 15.951622\n",
       "6 9ac8c3edbfc4a5aa340d91886a73d588 Advertising             9.253095  4.606095\n",
       "  R3       \n",
       "1  8.091069\n",
       "2  6.252373\n",
       "3  5.613295\n",
       "4  8.680847\n",
       "5 11.223350\n",
       "6 10.748271"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "head(df_parquet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "4431632"
      ],
      "text/latex": [
       "4431632"
      ],
      "text/markdown": [
       "4431632"
      ],
      "text/plain": [
       "[1] 4431632"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Contamos los registros del dataset ... es pequeño, no es BigData...\n",
    "count(df_parquet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aplicamos unas transformaciones sencillas al SparkDataFrame, copiando la tabla en una Vista Temporal para poder trabajar con ella en SQL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Creamos una vista SparkDataFrame con el nombre \"tmp_parquet\".\n",
    "# Este nombre tmp_parquet es el nombre que se usará ahora.\n",
    "createOrReplaceTempView(df_parquet,\"tmp_parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Una vista temporal, permite trabajar con una copia temporal de los datos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Contamos el número de registros:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Usamos SparkSQL para hacer consultas a los datos.\n",
    "count_rows <- sql(\"SELECT user_id,count(user_id) as registers FROM tmp_parquet group by user_id\")\n",
    "# Cuidado como obtener las cosas en SparkR: ---> Nooooooooo !!!! ;)\n",
    "# print(collect(count_rows))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compara el tiempo la opción anterior y la siguiente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<thead><tr><th scope=col>user_id</th><th scope=col>num_users</th></tr></thead>\n",
       "<tbody>\n",
       "\t<tr><td>33a39f26353e0f3e7005587c29a003d0</td><td>3                               </td></tr>\n",
       "\t<tr><td>10238bec09a1e497dd163154bcc7d348</td><td>7                               </td></tr>\n",
       "\t<tr><td>1a74c36cdef8b4cc1bd571df80a3fe77</td><td>4                               </td></tr>\n",
       "\t<tr><td>6765378752d4267b406dc5c640829676</td><td>2                               </td></tr>\n",
       "\t<tr><td>29da1de651a905d1b9afa74f43df74b3</td><td>4                               </td></tr>\n",
       "\t<tr><td>a3a7755aaf44dc3006d3b74c7aba5298</td><td>4                               </td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "\\begin{tabular}{r|ll}\n",
       " user\\_id & num\\_users\\\\\n",
       "\\hline\n",
       "\t 33a39f26353e0f3e7005587c29a003d0 & 3                               \\\\\n",
       "\t 10238bec09a1e497dd163154bcc7d348 & 7                               \\\\\n",
       "\t 1a74c36cdef8b4cc1bd571df80a3fe77 & 4                               \\\\\n",
       "\t 6765378752d4267b406dc5c640829676 & 2                               \\\\\n",
       "\t 29da1de651a905d1b9afa74f43df74b3 & 4                               \\\\\n",
       "\t a3a7755aaf44dc3006d3b74c7aba5298 & 4                               \\\\\n",
       "\\end{tabular}\n"
      ],
      "text/plain": [
       "  user_id                          num_users\n",
       "1 33a39f26353e0f3e7005587c29a003d0 3        \n",
       "2 10238bec09a1e497dd163154bcc7d348 7        \n",
       "3 1a74c36cdef8b4cc1bd571df80a3fe77 4        \n",
       "4 6765378752d4267b406dc5c640829676 2        \n",
       "5 29da1de651a905d1b9afa74f43df74b3 4        \n",
       "6 a3a7755aaf44dc3006d3b74c7aba5298 4        "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "head(count_rows)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si usamos una vista temporal, está estará disponible durante toda la sesión a no ser que se elimine la vista temporal con `unpersist(....)`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Probamos con otro ejemplo, para saber las categorías que hay:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# createOrReplaceTempView(df_parquet,\"tmp_parquet\") --> No volvermos a cargarla!\n",
    "# Usamos SparkSQL para hacer consultas a los datos.\n",
    "categories <- sql(\"SELECT cat FROM tmp_parquet group by cat\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<thead><tr><th scope=col>cat</th></tr></thead>\n",
       "<tbody>\n",
       "\t<tr><td>Competitors            </td></tr>\n",
       "\t<tr><td>Science                </td></tr>\n",
       "\t<tr><td>Family_and_Parenting   </td></tr>\n",
       "\t<tr><td>Sports                 </td></tr>\n",
       "\t<tr><td>Careers                </td></tr>\n",
       "\t<tr><td>Reference_and_Education</td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "\\begin{tabular}{r|l}\n",
       " cat\\\\\n",
       "\\hline\n",
       "\t Competitors            \\\\\n",
       "\t Science                \\\\\n",
       "\t Family\\_and\\_Parenting   \\\\\n",
       "\t Sports                 \\\\\n",
       "\t Careers                \\\\\n",
       "\t Reference\\_and\\_Education\\\\\n",
       "\\end{tabular}\n"
      ],
      "text/plain": [
       "  cat                    \n",
       "1 Competitors            \n",
       "2 Science                \n",
       "3 Family_and_Parenting   \n",
       "4 Sports                 \n",
       "5 Careers                \n",
       "6 Reference_and_Education"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "head(categories)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "¿Cómo se calcularía el número de elementos de cada categoría?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# createOrReplaceTempView(df_parquet,\"tmp_parquet\")   --> No volvemos a cargarla !\n",
    "# Usamos SparkSQL para hacer consultas a los datos.\n",
    "categories_list <- sql(\"SELECT cat,count(user_id) as num_users FROM tmp_parquet group by cat\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "head(categories_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "¿Cuando usuarios distintos hay y que suma total tienen por usuario?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# createOrReplaceTempView(df_parquet,\"tmp_parquet\") --> No volvemos a cargarla!\n",
    "# Usamos SparkSQL para hacer consultas a los datos.\n",
    "table_summary <- sql(\"SELECT user_id,SUM(R1) as sum_index FROM tmp_parquet group by user_id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "count(table_summary)\n",
    "head(table_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ERROR",
     "evalue": "Error in unpersist(tmp_parquet): objeto 'tmp_parquet' no encontrado\n",
     "output_type": "error",
     "traceback": [
      "Error in unpersist(tmp_parquet): objeto 'tmp_parquet' no encontrado\nTraceback:\n",
      "1. unpersist(tmp_parquet)"
     ]
    }
   ],
   "source": [
    "unpersist(table_summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Escritura de los datos\n",
    "\n",
    "Al igual que con los otros formatos, se pueden exportar a cualquier otro."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Escritura del fichero de formato parquet a formato parquet\n",
    "write.df(finals, path = \"/root/TallerSparkR/datasets/results/finals.parquet\", source = \"parquet\", mode = \"overwrite\")\n",
    "\n",
    "# Escritura del fichero de formato csv a formato a CSV\n",
    "write.df(finals, path = \"/root/TallerSparkR/datasets/results/finals.csv\", source = \"csv\", mode = \"overwrite\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## JSON"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "JSON, acrónimo de JavaScript Object Notation, es un formato de texto ligero para el intercambio de datos. \n",
    "\n",
    "JSON es un subconjunto de la notación literal de objetos de JavaScript aunque hoy, debido a su amplia adopción como alternativa a XML, se considera un formato de lenguaje independiente.\n",
    "\n",
    "Es un formato actualmente muy utilizado ya que se ha impuesto como modelo para la entrada y salida de datos desde multiples y variados servicios web. Por ejemplo por citar varios:\n",
    "\n",
    "- Twitter. https://dev.twitter.com/rest/public \n",
    "- Google APIs\n",
    "- FaceBook API\n",
    "- ...\n",
    "\n",
    "Veamos en http://localhost:25980/tree/datasets como son los ficheros JSON por dentro.\n",
    "\n",
    "Es muy utilizado este formato para servicios web de información, donde lo que prima es la sencillez y versatilidad. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lectura de datos\n",
    "\n",
    "La sintaxis es la misma, pero varía el identificador del tipo de fuente, en este caso JSON."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "costumers <- read.df(\"/root/TallerSparkR/datasets/json/buy_costumers_amazon.json\", \"json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Revisamos la información del SparkDataFrame y su estructura."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<thead><tr><th scope=col>amount</th><th scope=col>buy_hours</th><th scope=col>credit_card</th><th scope=col>first_name</th><th scope=col>id</th><th scope=col>last_name</th></tr></thead>\n",
       "<tbody>\n",
       "\t<tr><td>2582.27         </td><td>9:41            </td><td>5010121228072899</td><td>Roy             </td><td>1               </td><td>Hunt            </td></tr>\n",
       "\t<tr><td>2244.43         </td><td>10:11           </td><td>3560573751972466</td><td>Martin          </td><td>2               </td><td>Robinson        </td></tr>\n",
       "\t<tr><td> 626.54         </td><td>10:07           </td><td>6381020418347013</td><td>Julie           </td><td>3               </td><td>Medina          </td></tr>\n",
       "\t<tr><td>3786.54         </td><td>18:15           </td><td>3537886797721600</td><td>Scott           </td><td>4               </td><td>Edwards         </td></tr>\n",
       "\t<tr><td>1620.38         </td><td>21:12           </td><td>3558329354058037</td><td>Amanda          </td><td>5               </td><td>Morrison        </td></tr>\n",
       "\t<tr><td> 326.73         </td><td>9:24            </td><td>5602231558845873</td><td>Kelly           </td><td>6               </td><td>Parker          </td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "\\begin{tabular}{r|llllll}\n",
       " amount & buy\\_hours & credit\\_card & first\\_name & id & last\\_name\\\\\n",
       "\\hline\n",
       "\t 2582.27          & 9:41             & 5010121228072899 & Roy              & 1                & Hunt            \\\\\n",
       "\t 2244.43          & 10:11            & 3560573751972466 & Martin           & 2                & Robinson        \\\\\n",
       "\t  626.54          & 10:07            & 6381020418347013 & Julie            & 3                & Medina          \\\\\n",
       "\t 3786.54          & 18:15            & 3537886797721600 & Scott            & 4                & Edwards         \\\\\n",
       "\t 1620.38          & 21:12            & 3558329354058037 & Amanda           & 5                & Morrison        \\\\\n",
       "\t  326.73          & 9:24             & 5602231558845873 & Kelly            & 6                & Parker          \\\\\n",
       "\\end{tabular}\n"
      ],
      "text/plain": [
       "  amount  buy_hours credit_card      first_name id last_name\n",
       "1 2582.27 9:41      5010121228072899 Roy        1  Hunt     \n",
       "2 2244.43 10:11     3560573751972466 Martin     2  Robinson \n",
       "3  626.54 10:07     6381020418347013 Julie      3  Medina   \n",
       "4 3786.54 18:15     3537886797721600 Scott      4  Edwards  \n",
       "5 1620.38 21:12     3558329354058037 Amanda     5  Morrison \n",
       "6  326.73 9:24      5602231558845873 Kelly      6  Parker   "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "1000"
      ],
      "text/latex": [
       "1000"
      ],
      "text/markdown": [
       "1000"
      ],
      "text/plain": [
       "[1] 1000"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- amount: double (nullable = true)\n",
      " |-- buy_hours: string (nullable = true)\n",
      " |-- credit_card: string (nullable = true)\n",
      " |-- first_name: string (nullable = true)\n",
      " |-- id: long (nullable = true)\n",
      " |-- last_name: string (nullable = true)\n"
     ]
    }
   ],
   "source": [
    "# Un resumen\n",
    "head(costumers)\n",
    "\n",
    "# El numero de registros\n",
    "count(costumers)\n",
    "\n",
    "# La estructura\n",
    "printSchema(costumers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**¿Por qué no se lee correctamente el JSON?** Revisamos el dataset http://localhost:25980/tree/datasets y arreglamos el error.\n",
    "\n",
    "La lectura de multiples ficheros es similar lo que ocurre con CSV, donde podemos indica que hay más archivos que queremos leer desde el directorio.\n",
    "\n",
    "Lectura desde varios ficheros JSON:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "costumers_double <- read.json(c(\"/root/TallerSparkR/datasets/json/buy_costumers_amazon.json\", \"/root/TallerSparkR/datasets/json/buy_costumers_amazon.json\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "¿Cuántos registros hay ahora?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "2000"
      ],
      "text/latex": [
       "2000"
      ],
      "text/markdown": [
       "2000"
      ],
      "text/plain": [
       "[1] 2000"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "count(costumers_double)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Escritura de datos a formato JSON\n",
    "\n",
    "Al igual que para los otros formatos se usa el mismo esquema para guardar datasets a disco."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "write.df(costumers_double, path = \"datasets/results/costumers.json\", source = \"json\", mode = \"overwrite\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como siempre cerramos la sesion de Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sparkR.session.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<HR>\n",
    "# Zona de pruebas del NOTEBOOK en SparkR\n",
    "![FooterSparkR](https://sites.google.com/site/manuparra/home/footer_SparkR_v2.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Escribe todas las pruebas en R que necesites a partir de aquí\n",
    "\n",
    "<HR>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "3.3.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
