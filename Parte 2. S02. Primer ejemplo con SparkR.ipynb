{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style='font-size:2.4em'>Procesamiento masivo de datos con SparkR</span>\n",
    "\n",
    "<span style='font-size:1.5em'>VIII Jornadas de usuarios de R. Albacete, Castilla-La Mancha, 17 y 18 de noviembre de 2016</span>\n",
    "\n",
    "Taller impartido por: <span style='font-size:1.2em'>Manuel Jesús Parra Royón</span>\n",
    "\n",
    "\n",
    "![Alt](https://sites.google.com/site/manuparra/home/logoparty.png)\n",
    "\n",
    "<HR>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Primer ejemplo  y toma de contacto con SparkR\n",
    "\n",
    "![Spark+R](https://sites.google.com/site/manuparra/home/SparkRlogo.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como siempre para todos nuestros `scripts` con **SparkR**, cargamos la biblioteca, y creamos una nueva sesión de SparkR."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Attaching package: ‘SparkR’\n",
      "\n",
      "The following objects are masked from ‘package:stats’:\n",
      "\n",
      "    cov, filter, lag, na.omit, predict, sd, var, window\n",
      "\n",
      "The following objects are masked from ‘package:base’:\n",
      "\n",
      "    as.data.frame, colnames, colnames<-, drop, endsWith, intersect,\n",
      "    rank, rbind, sample, startsWith, subset, summary, transform, union\n",
      "\n",
      "Spark package found in SPARK_HOME: /usr/local/spark/\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Launching java with spark-submit command /usr/local/spark//bin/spark-submit   --driver-memory \"1g\" sparkr-shell /tmp/RtmpvIVTAw/backend_port86c74cf223a \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Java ref type org.apache.spark.sql.SparkSession id 1 "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Cargamos el path donde estará la biblioteca\n",
    ".libPaths(c(file.path(Sys.getenv(\"SPARK_HOME\"),\"R/lib/\"),.libPaths()))\n",
    "# Añadimos la bibioteca\n",
    "library(SparkR)\n",
    "# Abrimos la conexión\n",
    "sparkR.session(appName=\"Primeros_Pasos\", master = \"local[*]\", sparkConfig = list(spark.driver.memory = \"1g\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Con **SparkR** podemos crear un `DataFrame` de Spark desde un `data.frame` habitual usado en R. \n",
    "\n",
    "Un ``DataFrame`` es una colección distribuida de datos organizada en columnas. \n",
    "\n",
    "Los ``dataframes`` son conceptualmente equivalentes a bases de datos relacionales o a `data.frames` en R o Python, pero con una ventaja: son mucho más eficientes para el trabajo con grandes volumenes de datos. Los ``DataFrames``pueden ser creados desde una amplio surtido de fuentes muy diferentes. Es decir, casi cualquier cosa puede ser un ``dataframe``, por ejemplo ficheros estructurados, tablas en ``HIVE``, bases de datos externas o RDDs.\n",
    "\n",
    "\n",
    "Los RDDs son la principal abstracción de datos en Spark. Un RDD es una colección resilente y distribuida de registros. Esta es una de las claves de Spark y es uno de los componentes fundamentales del `core` de Spark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "'data.frame'"
      ],
      "text/latex": [
       "'data.frame'"
      ],
      "text/markdown": [
       "'data.frame'"
      ],
      "text/plain": [
       "[1] \"data.frame\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "'SparkDataFrame'"
      ],
      "text/latex": [
       "'SparkDataFrame'"
      ],
      "text/markdown": [
       "'SparkDataFrame'"
      ],
      "text/plain": [
       "[1] \"SparkDataFrame\"\n",
       "attr(,\"package\")\n",
       "[1] \"SparkR\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<thead><tr><th scope=col>eruptions</th><th scope=col>waiting</th></tr></thead>\n",
       "<tbody>\n",
       "\t<tr><td>3.600</td><td>79   </td></tr>\n",
       "\t<tr><td>1.800</td><td>54   </td></tr>\n",
       "\t<tr><td>3.333</td><td>74   </td></tr>\n",
       "\t<tr><td>2.283</td><td>62   </td></tr>\n",
       "\t<tr><td>4.533</td><td>85   </td></tr>\n",
       "\t<tr><td>2.883</td><td>55   </td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "\\begin{tabular}{r|ll}\n",
       " eruptions & waiting\\\\\n",
       "\\hline\n",
       "\t 3.600 & 79   \\\\\n",
       "\t 1.800 & 54   \\\\\n",
       "\t 3.333 & 74   \\\\\n",
       "\t 2.283 & 62   \\\\\n",
       "\t 4.533 & 85   \\\\\n",
       "\t 2.883 & 55   \\\\\n",
       "\\end{tabular}\n"
      ],
      "text/plain": [
       "  eruptions waiting\n",
       "1 3.600     79     \n",
       "2 1.800     54     \n",
       "3 3.333     74     \n",
       "4 2.283     62     \n",
       "5 4.533     85     \n",
       "6 2.883     55     "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- eruptions: double (nullable = true)\n",
      " |-- waiting: double (nullable = true)\n"
     ]
    }
   ],
   "source": [
    "# Vamos a usar un dataset sencillo integrado en R\n",
    "# El dataset contiene el tiempo de espera entre erupciones y duración \n",
    "# de la erupción de un geiser de Yellowstone\n",
    "class(faithful)\n",
    "\n",
    "# Convertimos un dataframe de R en un DataFrame de Spark, que llamaremos SparkDataFrame\n",
    "df_faithful <- createDataFrame(faithful)\n",
    "\n",
    "# Vemos el tipo de dataset nuevo\n",
    "class(df_faithful)\n",
    "\n",
    "# Visualizamos de forma rápida el contenido\n",
    "head(df_faithful)\n",
    "\n",
    "# Usamos la función printSchema de SparkR para 'deducir' el esquema de datos (la estructura)\n",
    "printSchema(df_faithful)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Un ``SparkDataFrame`` puede ser registrado como una vista temporal en ``SparkSQL`` y que permite ejecutar sentencias SQL sobre los datos. La funcionalidad de SQL permite a las aplicaciones y flujos de trabajo ejecutar consultas SQL de forma programatica, devolviendo el resultado también como SparkDataFrame.\n",
    "\n",
    "Esto es importante, ya que todas las transformaciones a los conjuntos de datos que están en formato SparkDataFrames, siguen siendo SparkDataFrames, lo que hace que toda su manipulación corra por parte de Spark con todas las ventajas que eso tiene:\n",
    "\n",
    "- Volumen masivo de datos\n",
    "- Almacenamiento distribuido\n",
    "- Resilencia\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Operaciones sencillas con SparkR sobre SparkDataFrames"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En estos ejemplos vamos a tratar de ver una parte muy muy simple sobre la manipulación de los datos en el formato que entiende SparkR."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "175"
      ],
      "text/latex": [
       "175"
      ],
      "text/markdown": [
       "175"
      ],
      "text/plain": [
       "[1] 175"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "175"
      ],
      "text/latex": [
       "175"
      ],
      "text/markdown": [
       "175"
      ],
      "text/plain": [
       "[1] 175"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<thead><tr><th scope=col>eruptions</th></tr></thead>\n",
       "<tbody>\n",
       "\t<tr><td>3.600</td></tr>\n",
       "\t<tr><td>3.333</td></tr>\n",
       "\t<tr><td>4.533</td></tr>\n",
       "\t<tr><td>4.700</td></tr>\n",
       "\t<tr><td>3.600</td></tr>\n",
       "\t<tr><td>4.350</td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "\\begin{tabular}{r|l}\n",
       " eruptions\\\\\n",
       "\\hline\n",
       "\t 3.600\\\\\n",
       "\t 3.333\\\\\n",
       "\t 4.533\\\\\n",
       "\t 4.700\\\\\n",
       "\t 3.600\\\\\n",
       "\t 4.350\\\\\n",
       "\\end{tabular}\n"
      ],
      "text/plain": [
       "  eruptions\n",
       "1 3.600    \n",
       "2 3.333    \n",
       "3 4.533    \n",
       "4 4.700    \n",
       "5 3.600    \n",
       "6 4.350    "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Contamos los elementos a partir de un filtro normal\n",
    "count(filter(df_faithful,\"eruptions>3.0\"))\n",
    "\n",
    "# Convertimos a vista temporal de datos en SparkSQL y le damos el nombre faithful a la 'tabla'\n",
    "createOrReplaceTempView(df_faithful,\"faithful\")\n",
    "\n",
    "# Usamos SparkSQL para hacer consultas a los datos.\n",
    "eruptions_sql <- sql(\"SELECT eruptions FROM faithful WHERE eruptions >= 3.0\")\n",
    "\n",
    "# Contamos el resultado\n",
    "count(eruptions_sql)\n",
    "\n",
    "# Mostramos un resumen\n",
    "head(eruptions_sql)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Operaciones sobre los conjuntos de datos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La guía de referencia de funciones de la API de Spark con R se puede ver en: \n",
    "\n",
    "    https://spark.apache.org/docs/2.0.0-preview/api/R/\n",
    "\n",
    "Veamos las más importantes y sus diferencias con las de R equivalentes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recordamos que siempre para trabajar con SparkR, tenemos que terminar la sesión de Spark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sparkR.session.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Con esta sentencia se cierra el contexto abierto en SparkR y se liberan todos los recursos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<HR>\n",
    "# Zona de pruebas del NOTEBOOK en SparkR\n",
    "![FooterSparkR](https://sites.google.com/site/manuparra/home/footer_SparkR_v2.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Escribe todas las pruebas en R que necesites a partir de aquí\n",
    "\n",
    "<HR>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "3.3.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
